# ===============================
# services/llm-service/utils/gpu_manager.py
# ===============================
# Ù…Ø¯ÛŒØ±ÛŒØª GPU Ø¨Ø±Ø§ÛŒ Ø³Ø±ÙˆÛŒØ³ LLM
# Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² GPU Client Ø¨Ø±Ø§ÛŒ coordination Ø¨Ø§ Ø³Ø±ÙˆÛŒØ³ GPU coordinator

import os
import logging
import torch
import psutil
from typing import Dict, Optional, Any
import asyncio

logger = logging.getLogger(__name__)

try:
    import GPUtil

    GPUTIL_AVAILABLE = True
except ImportError:
    GPUTIL_AVAILABLE = False
    logger.warning("âš ï¸ GPUtil not available - limited GPU monitoring")

try:
    from .gpu_client import SimpleGPUClient, GPUClientError, managed_gpu

    GPU_CLIENT_AVAILABLE = True
except ImportError:
    GPU_CLIENT_AVAILABLE = False
    logger.warning("âš ï¸ GPU Client not available - direct GPU access only")


class GPUManager:
    """Ù…Ø¯ÛŒØ±ÛŒØª GPU Ø¨Ø±Ø§ÛŒ Ø³Ø±ÙˆÛŒØ³ LLM"""

    def __init__(self, service_name: str = "llm-service"):
        self.service_name = service_name
        self.gpu_available = self._check_gpu_availability()
        self.gpu_client: Optional[SimpleGPUClient] = None
        self.current_gpu_id: Optional[int] = None

        logger.info(f"ğŸ® GPU Manager initialized - GPU Available: {self.gpu_available}")

        if self.gpu_available:
            self._log_gpu_info()

    def _check_gpu_availability(self) -> bool:
        """Ø¨Ø±Ø±Ø³ÛŒ Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ø¨ÙˆØ¯Ù† GPU"""
        try:
            # Ø¨Ø±Ø±Ø³ÛŒ PyTorch CUDA
            if not torch.cuda.is_available():
                logger.info("ğŸš« CUDA not available in PyTorch")
                return False

            # Ø¨Ø±Ø±Ø³ÛŒ ØªØ¹Ø¯Ø§Ø¯ GPU Ù‡Ø§
            gpu_count = torch.cuda.device_count()
            if gpu_count == 0:
                logger.info("ğŸš« No CUDA devices found")
                return False

            logger.info(f"âœ… Found {gpu_count} CUDA device(s)")
            return True

        except Exception as e:
            logger.error(f"âŒ GPU availability check failed: {e}")
            return False

    def _log_gpu_info(self):
        """Ù„Ø§Ú¯ Ø§Ø·Ù„Ø§Ø¹Ø§Øª GPU"""
        if not self.gpu_available:
            return

        try:
            for i in range(torch.cuda.device_count()):
                gpu_name = torch.cuda.get_device_name(i)
                gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
                logger.info(f"ğŸ® GPU {i}: {gpu_name} ({gpu_memory:.1f}GB)")

        except Exception as e:
            logger.warning(f"âš ï¸ Could not log GPU info: {e}")

    def get_gpu_info(self) -> Dict[str, Any]:
        """Ø¯Ø±ÛŒØ§ÙØª Ø§Ø·Ù„Ø§Ø¹Ø§Øª GPU"""
        if not self.gpu_available:
            return {
                "available": False,
                "count": 0,
                "current_gpu": None,
                "memory_info": {},
                "utilization": 0,
            }

        try:
            gpu_count = torch.cuda.device_count()
            current_gpu = (
                torch.cuda.current_device() if torch.cuda.is_available() else None
            )

            # Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø­Ø§ÙØ¸Ù‡ GPU ÙØ¹Ù„ÛŒ
            memory_info = {}
            utilization = 0

            if current_gpu is not None:
                memory_allocated = torch.cuda.memory_allocated(current_gpu) / 1024**3
                memory_reserved = torch.cuda.memory_reserved(current_gpu) / 1024**3
                memory_total = (
                    torch.cuda.get_device_properties(current_gpu).total_memory / 1024**3
                )

                memory_info = {
                    "allocated_gb": round(memory_allocated, 2),
                    "reserved_gb": round(memory_reserved, 2),
                    "total_gb": round(memory_total, 2),
                    "utilization_percent": round(
                        (memory_allocated / memory_total) * 100, 1
                    ),
                }
                utilization = memory_info["utilization_percent"]

            # Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø§Ø¶Ø§ÙÛŒ Ø¨Ø§ GPUtil Ø§Ú¯Ø± Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø§Ø´Ø¯
            gpu_details = []
            if GPUTIL_AVAILABLE:
                try:
                    gpus = GPUtil.getGPUs()
                    for gpu in gpus:
                        gpu_details.append(
                            {
                                "id": gpu.id,
                                "name": gpu.name,
                                "load": round(gpu.load * 100, 1),
                                "memory_used": round(gpu.memoryUsed / 1024, 2),
                                "memory_total": round(gpu.memoryTotal / 1024, 2),
                                "temperature": gpu.temperature
                                if hasattr(gpu, "temperature")
                                else None,
                            }
                        )
                except Exception as e:
                    logger.warning(f"âš ï¸ GPUtil info failed: {e}")

            return {
                "available": True,
                "count": gpu_count,
                "current_gpu": current_gpu,
                "coordinator_gpu": self.current_gpu_id,  # GPU Ø§Ø² coordinator
                "memory_info": memory_info,
                "utilization": utilization,
                "gpu_details": gpu_details,
            }

        except Exception as e:
            logger.error(f"âŒ GPU info error: {e}")
            return {"available": False, "error": str(e)}

    async def request_gpu_via_coordinator(
        self, memory_gb: float = 3.0, priority: str = "normal", max_wait_time: int = 120
    ) -> bool:
        """Ø¯Ø±Ø®ÙˆØ§Ø³Øª GPU Ø§Ø² Ø·Ø±ÛŒÙ‚ coordinator"""

        if not GPU_CLIENT_AVAILABLE:
            logger.warning("âš ï¸ GPU Client not available - using direct GPU access")
            return self.gpu_available

        try:
            coordinator_url = os.getenv(
                "GPU_COORDINATOR_URL", "https://gpu-coordinator:8080"
            )

            self.gpu_client = SimpleGPUClient(
                coordinator_url=coordinator_url, service_name=self.service_name
            )

            # Ø¨Ø±Ø±Ø³ÛŒ Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ø¨ÙˆØ¯Ù† coordinator
            if not await self.gpu_client.is_coordinator_available():
                logger.warning("âš ï¸ GPU Coordinator not available")
                return self.gpu_available

            # Ø¯Ø±Ø®ÙˆØ§Ø³Øª GPU
            allocation = await self.gpu_client.wait_for_gpu(
                memory_gb=memory_gb, priority=priority, max_wait_time=max_wait_time
            )

            if allocation.allocated:
                self.current_gpu_id = allocation.gpu_id
                os.environ["CUDA_VISIBLE_DEVICES"] = str(allocation.gpu_id)
                logger.info(f"âœ… GPU {allocation.gpu_id} allocated via coordinator")
                return True
            else:
                logger.warning("âš ï¸ GPU allocation failed")
                return False

        except Exception as e:
            logger.error(f"âŒ GPU coordinator request failed: {e}")
            return self.gpu_available

    async def release_gpu_from_coordinator(self) -> bool:
        """Ø¢Ø²Ø§Ø¯Ø³Ø§Ø²ÛŒ GPU Ø§Ø² coordinator"""
        if self.gpu_client and self.gpu_client.is_gpu_allocated():
            try:
                await self.gpu_client.release_gpu()
                self.current_gpu_id = None
                if "CUDA_VISIBLE_DEVICES" in os.environ:
                    del os.environ["CUDA_VISIBLE_DEVICES"]
                logger.info("âœ… GPU released from coordinator")
                return True
            except Exception as e:
                logger.error(f"âŒ GPU release failed: {e}")
                return False
        return True

    def set_gpu_device(self, device_id: int = 0):
        """ØªÙ†Ø¸ÛŒÙ… GPU device Ù…ÙˆØ±Ø¯ Ø§Ø³ØªÙØ§Ø¯Ù‡"""
        if not self.gpu_available:
            logger.warning("âš ï¸ No GPU available to set")
            return False

        try:
            if device_id >= torch.cuda.device_count():
                logger.error(f"âŒ GPU {device_id} not available")
                return False

            torch.cuda.set_device(device_id)
            logger.info(f"ğŸ® GPU device set to {device_id}")
            return True

        except Exception as e:
            logger.error(f"âŒ Failed to set GPU device: {e}")
            return False

    def clear_gpu_memory(self):
        """Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø­Ø§ÙØ¸Ù‡ GPU"""
        if not self.gpu_available:
            return

        try:
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
                logger.info("ğŸ§¹ GPU memory cleared")

        except Exception as e:
            logger.warning(f"âš ï¸ GPU memory clear failed: {e}")

    def get_memory_usage(self) -> Dict[str, float]:
        """Ø¯Ø±ÛŒØ§ÙØª Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø­Ø§ÙØ¸Ù‡ Ø³ÛŒØ³ØªÙ… Ùˆ GPU"""
        # Ø­Ø§ÙØ¸Ù‡ Ø³ÛŒØ³ØªÙ…
        memory = psutil.virtual_memory()
        result = {
            "system_memory_gb": round(memory.total / 1024**3, 2),
            "system_used_gb": round(memory.used / 1024**3, 2),
            "system_available_gb": round(memory.available / 1024**3, 2),
            "system_usage_percent": memory.percent,
        }

        # Ø­Ø§ÙØ¸Ù‡ GPU
        if self.gpu_available:
            try:
                current_gpu = torch.cuda.current_device()
                memory_allocated = torch.cuda.memory_allocated(current_gpu) / 1024**3
                memory_reserved = torch.cuda.memory_reserved(current_gpu) / 1024**3
                memory_total = (
                    torch.cuda.get_device_properties(current_gpu).total_memory / 1024**3
                )

                result.update(
                    {
                        "gpu_memory_total_gb": round(memory_total, 2),
                        "gpu_memory_allocated_gb": round(memory_allocated, 2),
                        "gpu_memory_reserved_gb": round(memory_reserved, 2),
                        "gpu_memory_free_gb": round(memory_total - memory_reserved, 2),
                        "gpu_usage_percent": round(
                            (memory_allocated / memory_total) * 100, 1
                        ),
                    }
                )

            except Exception as e:
                logger.warning(f"âš ï¸ GPU memory info failed: {e}")

        return result

    def get_optimal_batch_size(self, base_batch_size: int = 1) -> int:
        """ØªØ®Ù…ÛŒÙ† batch size Ø¨Ù‡ÛŒÙ†Ù‡ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø­Ø§ÙØ¸Ù‡ Ù…ÙˆØ¬ÙˆØ¯"""
        if not self.gpu_available:
            return base_batch_size

        try:
            current_gpu = torch.cuda.current_device()
            memory_total = torch.cuda.get_device_properties(current_gpu).total_memory
            memory_allocated = torch.cuda.memory_allocated(current_gpu)
            memory_free = memory_total - memory_allocated

            # ØªØ®Ù…ÛŒÙ† ØªÙ‚Ø±ÛŒØ¨ÛŒ: Ù‡Ø± batch Ø­Ø¯ÙˆØ¯ 1GB Ø­Ø§ÙØ¸Ù‡ Ù†ÛŒØ§Ø² Ø¯Ø§Ø±Ø¯
            estimated_batch_size = max(1, int(memory_free / (1024**3)))

            # Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø±Ø¯Ù† Ø¨Ù‡ ÛŒÚ© Ø­Ø¯ Ù…Ù†Ø·Ù‚ÛŒ
            optimal_batch_size = min(base_batch_size * 2, estimated_batch_size)

            logger.info(f"ğŸ¯ Optimal batch size: {optimal_batch_size}")
            return optimal_batch_size

        except Exception as e:
            logger.warning(f"âš ï¸ Batch size estimation failed: {e}")
            return base_batch_size

    async def __aenter__(self):
        """Context manager entry"""
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Context manager exit with cleanup"""
        await self.release_gpu_from_coordinator()
        self.clear_gpu_memory()


# =======================
# Utility Functions
# =======================


def get_gpu_info_summary() -> Dict[str, Any]:
    """Ø¯Ø±ÛŒØ§ÙØª Ø®Ù„Ø§ØµÙ‡ Ø§Ø·Ù„Ø§Ø¹Ø§Øª GPU"""
    manager = GPUManager()
    return manager.get_gpu_info()


def check_gpu_compatibility() -> bool:
    """Ø¨Ø±Ø±Ø³ÛŒ Ø³Ø§Ø²Ú¯Ø§Ø±ÛŒ GPU"""
    if not torch.cuda.is_available():
        return False

    try:
        # ØªØ³Øª Ø³Ø§Ø¯Ù‡ GPU
        device = torch.cuda.current_device()
        tensor = torch.tensor([1.0], device=device)
        result = tensor * 2
        return result.item() == 2.0

    except Exception as e:
        logger.error(f"âŒ GPU compatibility test failed: {e}")
        return False


# =======================
# Example Usage
# =======================


async def example_gpu_management():
    """Ù†Ù…ÙˆÙ†Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² GPU Manager"""

    async with GPUManager("example-service") as gpu_manager:
        # Ø¯Ø±ÛŒØ§ÙØª Ø§Ø·Ù„Ø§Ø¹Ø§Øª GPU
        info = gpu_manager.get_gpu_info()
        print(f"GPU Info: {info}")

        # Ø¯Ø±Ø®ÙˆØ§Ø³Øª GPU Ø§Ø² coordinator
        if await gpu_manager.request_gpu_via_coordinator(memory_gb=2.0):
            print("âœ… GPU allocated via coordinator")

            # Ø§Ù†Ø¬Ø§Ù… Ú©Ø§Ø± Ø¨Ø§ GPU
            await asyncio.sleep(2)

        # Ø¢Ø²Ø§Ø¯Ø³Ø§Ø²ÛŒ Ø®ÙˆØ¯Ú©Ø§Ø± Ø¯Ø± __aexit__


if __name__ == "__main__":
    # ØªØ³Øª GPU Manager
    logging.basicConfig(level=logging.INFO)
    asyncio.run(example_gpu_management())
