-------------Docker Compose Container:------------

ุณุฑูุณ ุฌุฏุฏ llm-service:

ฺฉุงูุชูุฑ ุฏุฑ ูพูุฑุช 8002 ุงุฌุฑุง ูโุดูุฏ
ูุฏู gpt2-fa ุงุฒ ูุณุฑ /data/models/llm/gpt2-fa mount ูโุดูุฏ
ูุชุบุฑูุง ูุญุท ูุฎุตูุต ูุฏู ุฒุจุงู ุชูุธู ุดุฏู
ูุญุฏูุฏุช ุญุงูุธู (4GB ุญุฏุงฺฉุซุฑุ 2GB ุฑุฒุฑู) ุจุฑุง ูุฏุฑุช ุจูุชุฑ ููุงุจุน
Health check ุจุง timeout ุจุดุชุฑ (30 ุซุงูู) ุจุฑุง ูุฏูโูุง ุณูฺฏู

ุชุบุฑุงุช ุฏุฑ ุณุฑูุณโูุง ููุฌูุฏ:

ุฏุฑ api-gateway ู test-service ูุชุบุฑ LLM_SERVICE_URL ุงุถุงูู ุดุฏ
ูุงุจุณุชฺฏ ุจู llm-service ุฏุฑ ุณุฑูุณโูุง ูุฑุจูุทู ุชุนุฑู ุดุฏ

ูฺฉุงุช ููู:

Volume ูุฏู ุจู ุตูุฑุช read-only mount ุดุฏู (:ro)
Health check ุจุฑุง LLM service ุฒูุงู ุจุดุชุฑ ุฏุงุฑุฏ ฺูู ููุฏ ูุฏู ุฒูุงูโุจุฑ ุงุณุช
ูุญุฏูุฏุช ุญุงูุธู ุจุฑุง ุฌููฺฏุฑ ุงุฒ ูุตุฑู ุจุด ุงุฒ ุญุฏ ููุงุจุน ุชูุธู ุดุฏู

ุญุงูุง ูโุชูุงูุฏ Dockerfile ู ฺฉุฏ ุณุฑูุณ LLM ุฑุง ุจุณุงุฒุฏ ู ฺฉุงูุชูุฑ ุฑุง ุงุฌุฑุง ฺฉูุฏ.

-------------nginx.conf--------------------
nginx.conf ุจูโุฑูุฒุฑุณุงู ุดุฏ. ุชุบุฑุงุช ุงุตู:
๐ง ุชุบุฑุงุช ฺฉูุฏ:

Upstream ุฌุฏุฏ: llm_service ุจุฑุง ุณุฑูุณ LLM ุงุถุงูู ุดุฏ
Rate Limiting:

llm_limit ุจุง ูุญุฏูุฏุช 2 ุฏุฑุฎูุงุณุช ุฏุฑ ุซุงูู (ฺฉูุชุฑ ุงุฒ API ุนุงุฏ)
Burst ุชุง 5 ุฏุฑุฎูุงุณุช ุจุฑุง LLM

ูุณุฑูุง ุฌุฏุฏ:

/api/llm/ โ ูุฏุงุช ุจู llm-service
/llm/health โ health check ูุฎุตูุต LLM

ุชูุธูุงุช ุฎุงุต LLM:

Timeout ุจุดุชุฑ: 120 ุซุงูู ุจุฑุง ูพุฑุฏุงุฒุด ูุฏูโูุง ุณูฺฏู
Buffer ุจุฒุฑฺฏโุชุฑ: ุจุฑุง ูพุงุณุฎโูุง ุทููุงู (128k/256k)
WebSocket Support: ุจุฑุง streaming responses

๐ฏ ูุณุฑูุง ุฏุฑ ุฏุณุชุฑุณ:

GET /llm/health โ ุจุฑุฑุณ ุณูุงูุช ุณุฑูุณ LLM
POST /api/llm/generate โ ุชููุฏ ูุชู
POST /api/llm/chat โ ฺฏูุชฺฏู ุจุง ูุฏู
ุณุงุฑ endpoint ูุง LLM

โก ุจูููโุณุงุฒ ุนููฺฉุฑุฏ:

Timeout ูุง ููุงุณุจ ุจุฑุง ุฏุฑุฎูุงุณุชโูุง ุณูฺฏู
Buffer ูุง ุจุฒุฑฺฏ ุจุฑุง ูพุงุณุฎโูุง ุทููุงู
Rate limiting ููุงุณุจ ุจุฑุง ูุญุงูุธุช ุงุฒ ููุงุจุน

ุญุงูุง nginx ุขูุงุฏู ฺฉุงุฑ ุจุง ุณุฑูุณ LLM ุงุณุช!

----------------------------prometheus.yml---------------
prometheus.yml ุจูโุฑูุฒุฑุณุงู ุดุฏ. ุชุบุฑุงุช ุงุตู:
๐ง ุณุฑูุณ ุฌุฏุฏ ุงุถุงูู ุดุฏู:

llm-service job:

Target: llm-service:8002
Scrape interval: 60 ุซุงูู (ุจุดุชุฑ ุงุฒ ุณุฑูุณโูุง ุนุงุฏ)
Timeout: 30 ุซุงูู (ุจุฑุง metrics ุณูฺฏู)
Params ุฎุงุต ุจุฑุง metrics ูุฏู ุฒุจุงู

llm-health job:

ูุงูุชูุฑูฺฏ ุฌุฏุงฺฏุงูู ุจุฑุง ุณูุงูุช ุณุฑูุณ
Interval: 120 ุซุงูู (ูุฑ 2 ุฏููู)
Timeout: 45 ุซุงูู

๐ Metrics ููุฑุฏ ุงูุชุธุงุฑ ุงุฒ LLM Service:

model_inference_duration - ุฒูุงู ุงุณุชูุชุงุฌ
model_memory_usage - ูุตุฑู ุญุงูุธู
model_requests_total - ุชุนุฏุงุฏ ุฏุฑุฎูุงุณุชโูุง
model_errors_total - ุชุนุฏุงุฏ ุฎุทุงูุง
gpu_utilization - ุงุณุชูุงุฏู ุงุฒ GPU (ุงฺฏุฑ ูุฌูุฏ ุฏุงุดุชู ุจุงุดุฏ)
model_queue_size - ุงูุฏุงุฒู ุตู ุฏุฑุฎูุงุณุชโูุง

โก ุจูููโุณุงุฒ:

ููุงุตู ุฒูุงู ุจุดุชุฑ ุจุฑุง ุฌููฺฏุฑ ุงุฒ ุงุฎุชูุงู ุฏุฑ ฺฉุงุฑฺฉุฑุฏ ูุฏู
Timeout ูุง ููุงุณุจ ุจุฑุง ุนููุงุช ุณูฺฏู
ุฌุฏุงุณุงุฒ health check ุงุฒ metrics ุงุตู

ุญุงูุง Prometheus ุขูุงุฏู ูุงูุชูุฑ ฺฉุฑุฏู ุณุฑูุณ LLM ุงุณุช!

---------------kole file haie llm-service ra ba version ghabl avaz kardam o saboktar kardam-------------------

----------------services/api-gateway/routes/llm.py--------------------
services/api-gateway/routes/llm.py ุฏุงุฑู ู ุฌุฏูู ูพูุฑุชโูุง ุฑุง ูุฒ ุงุตูุงุญ :
ู:

services/api-gateway/routes/llm.py - Route handler ุจุฑุง LLM endpoints
ุฌุฏูู ูพูุฑุชโูุง - ุจูโุฑูุฒุฑุณุงู ุดุฏู ุจุง LLM Service

๐ง ูฺฺฏโูุง Route Handler:

Health Check - ุจุฑุฑุณ ูุถุนุช ุณุฑูุณ
Text Generation - ุชููุฏ ูุชู ุจุง ูพุงุฑุงูุชุฑูุง ูุงุจู ุชูุธู
Chat Interface - ฺฏูุชฺฏู ุชุนุงูู
Model Info - ุงุทูุงุนุงุช ูุฏู
Batch Processing - ูพุฑุฏุงุฒุด ุฏุณุชูโุง (ุขูุงุฏู ุจุฑุง ุขูุฏู)
Error Handling - ูุฏุฑุช ุฎุทุงูุง ูุฎุชูู
Logging - ุซุจุช ุฏุฑุฎูุงุณุชโูุง ู ุนููฺฉุฑุฏ
Timeout Management - ูุฏุฑุช ุฒูุงู ุงูุชุธุงุฑ

๐ก ูฺฉุชู ููู:
ุจุฑุง ูุนุงูโุณุงุฒ ุงู route ูุงุ ุจุงุฏ ุฏุฑ ูุงู ุงุตู API Gateway (ูุนูููุงู main.py) ุงู router ุฑุง import ู ุงุถุงูู ฺฉูุฏ:

---------------------ูุงู main.py ุงุตูุงุญ ุฏูุจุงุฑู---------------
ุจุฎุงุทุฑ api gateway routes:
ู:
๐ง ุณุงุฒฺฏุงุฑ ุจุง API Gateway:

Chat Endpoint ุฌุฏุฏ:

POST /chat - ฺฏูุชฺฏู ุชุนุงูู ุจุง ูุฏุฑุช session
Chat session management ุจุง UUID
Context awareness ุจุฑุง ูฺฉุงููุงุช

Response Models ุจูุจูุฏ ุงูุชู:

GenerationResponse ุญุงูุง single string ุจุฑูโฺฏุฑุฏุงูุฏ (ูู list)
ChatResponse ูุทุงุจู ุจุง API Gateway
BatchGenerationResponse ุจุฑุง ูพุฑุฏุงุฒุด ุฏุณุชูโุง

Batch Processing:

POST /batch/generate - ูพุฑุฏุงุฒุด ููุฒูุงู ฺูุฏู ุฏุฑุฎูุงุณุช
ูุญุฏูุฏุช 10 ุฏุฑุฎูุงุณุช ุฏุฑ ูุฑ batch

๐ ูฺฺฏโูุง ุฌุฏุฏ:

Chat Session Management:

ุฐุฎุฑู ูฺฉุงููุงุช ุฏุฑ ุญุงูุธู
Auto-cleanup sessions ูุฏู (ุจุนุฏ ุงุฒ 1 ุณุงุนุช)
Context window ุจุฑุง ูฺฉุงููุงุช (ุขุฎุฑู 5 ูพุงู)

Enhanced Monitoring:

/health/metrics - metrics ุฎุงุต ุจุฑุง Prometheus
CHAT_SESSIONS gauge ุจุฑุง ุชุนุฏุงุฏ session ูุง ูุนุงู
ุจูุชุฑ ุดุฏู memory ู GPU monitoring

API Endpoints ุงุถุงู:

DELETE /chat/{conversation_id} - ุญุฐู session
ุจูุจูุฏ /model/info ุจุง ุงุทูุงุนุงุช ฺฉุงููโุชุฑ

๐ฏ ูุทุงุจูุช ฺฉุงูู:

ููู Response/Request models ูุทุงุจู ุจุง API Gateway
Error handling ฺฉุณุงู
Timeout ูุง ู ูุญุฏูุฏุชโูุง ูุดุงุจู
Metrics integration ฺฉุงูู

**\*\*\*\***\***\*\*\*\*** ูุงู ูุง ุฒุฑ ุงุฎุชุงุฑ **\*\***\*\***\*\***
ุงู ูุงู ูุง ุงุฎุชุงุฑ ุงุณุช ู ุจุฑุง ูุงุฒ ุจุนุฏ ุฏุฑ ูุธุฑ ูฺฏุฑู:
shared/models/request.py - ุงุถุงูู ฺฉุฑุฏู ูุฏูโูุง ุฏุฑุฎูุงุณุช LLM
shared/models/response.py - ุงุถุงูู ฺฉุฑุฏู ูุฏูโูุง ูพุงุณุฎ LLM

configs/models/llm_configs.yaml - ุชูุธูุงุช ูุฏูโูุง
llm-service/config/model_configs.yaml - ฺฉูพ ุงุฒ ุชูุธูุงุช
llm-service/utils/model_cache.py - ฺฉุด ูุฏูโูุง

ุงู ูุง ุฏุฑ ูููุฏุฑ ุฌุฏุง faz2/extra ูุฐุงุฑู:

---------ูุฏู ฺฉู ูุงู main.py---------
ุงู ูุงูุ ุณุฑูุณ LLM ุฑุง ุจุง ุงุณุชูุงุฏู ุงุฒ FastAPI ูพุงุฏูโุณุงุฒ ฺฉุฑุฏู ฺฉู ูโุชููู:

ูุฏู ุฒุจุงู ูุงุฑุณ (ูุซู GPT2-fa) ุฑู ุจุงุฑฺฏุฐุงุฑ ฺฉูู

ูุชู ุชููุฏ ฺฉูู (/generate)

ุญุงูุช ฺฏูุชโูฺฏู ุฏุงุดุชู ุจุงุดู (/chat)

ูุถุนุช ุณูุงูุช ุฑู ุงุฑุงุฆู ุจุฏู (/health, /metrics)

-----------------ุชุณุช--------------
ุฏุฑ cmd:
curl -X POST http://localhost:8002/generate -H "Content-Type: application/json" -d "{\"prompt\": \"ฺฺฉ ุดุนุฑ ุนุงุดูุงูู ฺูุงุฑ ุจุช ุฏุฑ ุณุจฺฉ ุญุงูุธ ุจููุณ\", \"max_length\": 400, \"temperature\":0.7}"

----todo:
model gpt2-fa aslan khob javab nemide , bejash ieki dige bezar
