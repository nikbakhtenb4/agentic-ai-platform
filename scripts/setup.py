#!/usr/bin/env python3
"""
Agentic AI Platform Setup Script
Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ù¾Ù„ØªÙØ±Ù… Ø¨Ø§ LLM Service
"""

import os
import sys
import subprocess
import time
import requests
from pathlib import Path

def print_banner():
    """Ù†Ù…Ø§ÛŒØ´ Ø¨Ù†Ø± Ø´Ø±ÙˆØ¹"""
    banner = """
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘        Agentic AI Platform           â•‘
    â•‘    Setup & Deployment with LLM       â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """
    print(banner)

def check_requirements():
    """Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ Docker Ùˆ Docker Compose"""
    print("ğŸ” Checking requirements...")
    
    try:
        subprocess.run(["docker", "--version"], check=True, capture_output=True)
        print("âœ… Docker is installed")
    except (subprocess.CalledProcessError, FileNotFoundError):
        print("âŒ Docker is not installed or not in PATH")
        return False
    
    try:
        subprocess.run(["docker", "compose", "version"], check=True, capture_output=True)
        print("âœ… Docker Compose is installed")
    except (subprocess.CalledProcessError, FileNotFoundError):
        print("âŒ Docker Compose is not installed or not in PATH")
        return False
    
    return True

def check_gpu_support():
    """Ø¨Ø±Ø±Ø³ÛŒ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ GPU"""
    print("ğŸ® Checking GPU support...")
    
    try:
        result = subprocess.run(["nvidia-smi"], capture_output=True, text=True)
        if result.returncode == 0:
            print("âœ… NVIDIA GPU detected")
            print("ğŸ’¡ LLM Service will use GPU acceleration")
            return True
        else:
            print("âš ï¸  No NVIDIA GPU detected - using CPU")
            return False
    except FileNotFoundError:
        print("âš ï¸  nvidia-smi not found - using CPU")
        return False

def check_llm_model():
    """Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ Ù…Ø¯Ù„ LLM"""
    print("ğŸ¤– Checking LLM model...")
    
    model_path = Path("data/models/llm/gpt2-fa")
    if not model_path.exists():
        print("âŒ LLM model directory not found!")
        print(f"   Expected path: {model_path.absolute()}")
        print("   Please ensure the GPT2-FA model is placed in the correct directory")
        return False
    
    # Ø¨Ø±Ø±Ø³ÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø² Ù…Ø¯Ù„
    required_files = [
        "config.json",
        "pytorch_model.bin",
        "tokenizer.json",
        "vocab.json"
    ]
    
    missing_files = []
    for file_name in required_files:
        file_path = model_path / file_name
        if not file_path.exists():
            missing_files.append(file_name)
    
    if missing_files:
        print(f"âš ï¸  Some model files might be missing: {', '.join(missing_files)}")
        print("   The model might still work, but please verify completeness")
    else:
        print("âœ… LLM model files found")
    
    # Ø¨Ø±Ø±Ø³ÛŒ Ø­Ø¬Ù… Ù…Ø¯Ù„
    try:
        total_size = sum(f.stat().st_size for f in model_path.rglob('*') if f.is_file())
        size_mb = total_size / (1024 * 1024)
        print(f"ğŸ“¦ Model size: {size_mb:.1f} MB")
        
        if size_mb < 10:
            print("âš ï¸  Model size seems small - please verify model integrity")
        
    except Exception as e:
        print(f"âš ï¸  Could not calculate model size: {e}")
    
    return True

def create_directories():
    """Ø§ÛŒØ¬Ø§Ø¯ Ø¯Ø§ÛŒØ±Ú©ØªÙˆØ±ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø²"""
    print("ğŸ“ Creating required directories...")
    
    directories = [
        "data/models/llm",
        "data/vectors", 
        "data/cache",
        "data/logs",
        "data/uploads",
        "monitoring/grafana/dashboards",
        "monitoring/grafana/provisioning/datasources",
        "services/api-gateway/middleware",
        "services/api-gateway/routes",
        "services/test-service",
        "services/llm-service/models",
        "services/llm-service/utils",
        "shared/database",
        "shared/models",
        "shared/utils"
    ]
    
    for directory in directories:
        Path(directory).mkdir(parents=True, exist_ok=True)
        print(f"âœ… Created: {directory}")

def setup_environment():
    """ØªÙ†Ø¸ÛŒÙ… ÙØ§ÛŒÙ„ environment"""
    print("ğŸ”§ Setting up environment...")
    
    if not os.path.exists(".env"):
        print("âŒ .env file not found! Please create it first.")
        return False
    
    print("âœ… Environment file exists")
    return True

def check_system_resources():
    """Ø¨Ø±Ø±Ø³ÛŒ Ù…Ù†Ø§Ø¨Ø¹ Ø³ÛŒØ³ØªÙ…"""
    print("ğŸ’¾ Checking system resources...")
    
    try:
        import psutil
        
        # Ø¨Ø±Ø±Ø³ÛŒ RAM
        memory = psutil.virtual_memory()
        memory_gb = memory.total / (1024**3)
        print(f"ğŸ Available RAM: {memory_gb:.1f} GB")
        
        if memory_gb < 4:
            print("âš ï¸  Warning: Less than 4GB RAM detected")
            print("   LLM Service might face memory issues")
        elif memory_gb >= 8:
            print("âœ… Sufficient RAM for LLM processing")
        else:
            print("âš ï¸  Moderate RAM - consider closing other applications")
        
        # Ø¨Ø±Ø±Ø³ÛŒ CPU
        cpu_count = psutil.cpu_count()
        print(f"ğŸ”§ CPU cores: {cpu_count}")
        
        if cpu_count < 4:
            print("âš ï¸  Warning: Less than 4 CPU cores detected")
            print("   LLM processing might be slow")
        
        # Ø¨Ø±Ø±Ø³ÛŒ ÙØ¶Ø§ÛŒ Ø¯ÛŒØ³Ú© (Ø¨Ø§ handling Ø¨Ø±Ø§ÛŒ ÙˆÛŒÙ†Ø¯ÙˆØ²)
        try:
            # Ø¯Ø± ÙˆÛŒÙ†Ø¯ÙˆØ² Ù…Ù…Ú©Ù† Ø§Ø³Øª Ù†ÛŒØ§Ø² Ø¨Ù‡ Ù…Ø³ÛŒØ± Ù…Ø·Ù„Ù‚ Ø¨Ø§Ø´Ø¯
            current_dir = os.path.abspath('.')
            disk = psutil.disk_usage(current_dir)
            free_gb = disk.free / (1024**3)
            print(f"ğŸ’¿ Free disk space: {free_gb:.1f} GB")
            
            if free_gb < 5:
                print("âš ï¸  Warning: Low disk space")
                print("   Consider freeing up space for model caching")
        except Exception as disk_error:
            print(f"âš ï¸  Could not check disk space: {disk_error}")
            print("   Please ensure sufficient disk space manually")
            
    except ImportError:
        print("âš ï¸  psutil not available - skipping resource check")
        print("   Install with: pip install psutil")
    except Exception as e:
        print(f"âš ï¸  Error checking system resources: {e}")
        print("   Continuing with setup...")

def build_services():
    """Ø³Ø§Ø®Øª Ø³Ø±ÙˆÛŒØ³â€ŒÙ‡Ø§"""
    print("ğŸ—ï¸  Building services...")
    
    # ØªÙ†Ø¸ÛŒÙ… encoding Ø¨Ø±Ø§ÛŒ ÙˆÛŒÙ†Ø¯ÙˆØ²
    env = os.environ.copy()
    env['PYTHONIOENCODING'] = 'utf-8'
    
    # Ø§Ø¨ØªØ¯Ø§ LLM Service Ø±Ø§ build Ú©Ù†ÛŒÙ… (Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø²Ù…Ø§Ù† Ø¨ÛŒØ´ØªØ±ÛŒ Ø¨Ø¨Ø±Ø¯)
    print("ğŸ¤– Building LLM Service...")
    try:
        # Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´ Ù„Ø§ÛŒÙˆ Ù„Ø§Ú¯â€ŒÙ‡Ø§
        process = subprocess.Popen(
            ["docker", "compose", "build", "llm-service", "--progress", "plain"],
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            universal_newlines=True,
            encoding='utf-8',
            errors='replace',
            env=env
        )
        
        # Ù†Ù…Ø§ÛŒØ´ Ø®Ø±ÙˆØ¬ÛŒ Ø¯Ø± real-time
        for line in process.stdout:
            if "Step" in line or "ERROR" in line or "FAILED" in line or "âœ“" in line:
                print(f"   {line.strip()}")
        
        process.wait()
        
        if process.returncode == 0:
            print("âœ… LLM Service built successfully")
        else:
            print("âŒ LLM Service build failed")
            print("ğŸ’¡ Trying alternative build method...")
            
            # ØªÙ„Ø§Ø´ Ù…Ø¬Ø¯Ø¯ Ø¨Ø§ Ø±ÙˆØ´ Ø³Ø§Ø¯Ù‡â€ŒØªØ±
            result = subprocess.run(
                ["docker", "compose", "build", "llm-service"],
                check=True,
                env=env
            )
            print("âœ… LLM Service built successfully (alternative method)")
            
    except subprocess.CalledProcessError as e:
        print(f"âŒ LLM Service build failed")
        print("ğŸ” Checking for common issues...")
        
        # Ø¨Ø±Ø±Ø³ÛŒ ÙØ§ÛŒÙ„ Dockerfile
        dockerfile_path = Path("services/llm-service/Dockerfile")
        if not dockerfile_path.exists():
            print("âŒ Dockerfile not found for LLM Service")
            return False
        
        # Ø¨Ø±Ø±Ø³ÛŒ requirements.txt
        req_path = Path("services/llm-service/requirements.txt")
        if not req_path.exists():
            print("âŒ requirements.txt not found for LLM Service")
            return False
            
        print("ğŸ’¡ Try building manually with: docker compose build llm-service --no-cache")
        return False
    
    # Ø³Ù¾Ø³ Ø¨Ù‚ÛŒÙ‡ Ø³Ø±ÙˆÛŒØ³â€ŒÙ‡Ø§ Ø±Ø§ build Ú©Ù†ÛŒÙ…
    print("ğŸ”§ Building other services...")
    try:
        result = subprocess.run(
            ["docker", "compose", "build"],
            check=True,
            env=env
        )
        print("âœ… All services built successfully")
        return True
    except subprocess.CalledProcessError as e:
        print(f"âŒ Build failed")
        print("ğŸ’¡ Try building manually with: docker compose build --no-cache")
        return False

def start_services():
    """Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø³Ø±ÙˆÛŒØ³â€ŒÙ‡Ø§"""
    print("ğŸš€ Starting services...")
    
    try:
        subprocess.run(
            ["docker", "compose", "up", "-d"],
            check=True
        )
        print("âœ… Services started")
        return True
    except subprocess.CalledProcessError as e:
        print(f"âŒ Failed to start services: {e}")
        return False

def wait_for_services():
    """Ø§Ù†ØªØ¸Ø§Ø± Ø¨Ø±Ø§ÛŒ Ø¢Ù…Ø§Ø¯Ù‡ Ø´Ø¯Ù† Ø³Ø±ÙˆÛŒØ³â€ŒÙ‡Ø§"""
    print("â³ Waiting for services to be ready...")
    
    services = {
        "API Gateway": "http://localhost:8000/health",
        "LLM Service": "http://localhost:8002/health", 
        "Test Service": "http://localhost:8001/health",
        "Prometheus": "http://localhost:9090/-/ready",
        "Grafana": "http://localhost:3000/api/health"
    }
    
    for service_name, url in services.items():
        print(f"ğŸ” Checking {service_name}...")
        
        # Ø¨Ø±Ø§ÛŒ LLM Service Ø²Ù…Ø§Ù† Ø¨ÛŒØ´ØªØ±ÛŒ Ø¯Ø± Ù†Ø¸Ø± Ù…ÛŒâ€ŒÚ¯ÛŒØ±ÛŒÙ…
        max_attempts = 60 if service_name == "LLM Service" else 30
        
        for attempt in range(max_attempts):
            try:
                response = requests.get(url, timeout=5)
                if response.status_code == 200:
                    print(f"âœ… {service_name} is ready")
                    break
            except requests.exceptions.RequestException:
                pass
            
            if attempt < max_attempts - 1:
                if service_name == "LLM Service" and attempt % 10 == 0:
                    print(f"   ğŸ’­ Still loading model... ({attempt}/{max_attempts})")
                time.sleep(2)
            else:
                print(f"âš ï¸  {service_name} might not be ready (timeout)")

def test_llm_service():
    """ØªØ³Øª LLM Service"""
    print("ğŸ§ª Testing LLM Service...")
    
    try:
        # ØªØ³Øª health endpoint
        response = requests.get("http://localhost:8002/health", timeout=10)
        if response.status_code == 200:
            print("âœ… LLM Health check passed")
        else:
            print(f"âš ï¸  LLM Health check returned: {response.status_code}")
            return False
        
        # ØªØ³Øª model info endpoint
        response = requests.get("http://localhost:8002/model/info", timeout=10)
        if response.status_code == 200:
            model_info = response.json()
            print(f"âœ… Model loaded: {model_info.get('model_name', 'Unknown')}")
            print(f"   ğŸ“Š Max length: {model_info.get('max_length', 'Unknown')}")
        else:
            print("âš ï¸  Could not retrieve model info")
        
        # ØªØ³Øª Ø³Ø§Ø¯Ù‡ ØªÙˆÙ„ÛŒØ¯ Ù…ØªÙ†
        test_payload = {
            "text": "Ø³Ù„Ø§Ù…",
            "max_length": 50,
            "temperature": 0.7
        }
        
        print("ğŸ”¬ Testing text generation...")
        response = requests.post(
            "http://localhost:8002/generate",
            json=test_payload,
            timeout=30
        )
        
        if response.status_code == 200:
            result = response.json()
            generated_text = result.get('generated_text', '')[:100]  # Ù†Ù…Ø§ÛŒØ´ 100 Ú©Ø§Ø±Ø§Ú©ØªØ± Ø§ÙˆÙ„
            print(f"âœ… Text generation test passed")
            print(f"   ğŸ’¬ Sample output: {generated_text}...")
        else:
            print(f"âš ï¸  Text generation test failed: {response.status_code}")
            if response.text:
                print(f"   Error: {response.text[:200]}")
        
        return True
        
    except requests.exceptions.RequestException as e:
        print(f"âŒ LLM Service test failed: {e}")
        return False

def test_endpoints():
    """ØªØ³Øª endpoint Ù‡Ø§ÛŒ Ø§ØµÙ„ÛŒ"""
    print("ğŸ§ª Testing endpoints...")
    
    endpoints = [
        ("Root", "http://localhost/"),
        ("API Info", "http://localhost/api/info"),
        ("Health Check", "http://localhost/health"),
        ("Test Service", "http://localhost/test/ping"),
        ("Connection Test", "http://localhost/api/test-connection"),
        ("LLM via Gateway", "http://localhost/api/llm/health")
    ]
    
    for name, url in endpoints:
        try:
            response = requests.get(url, timeout=5)
            if response.status_code == 200:
                print(f"âœ… {name}: OK")
            else:
                print(f"âš ï¸  {name}: Status {response.status_code}")
        except requests.exceptions.RequestException as e:
            print(f"âŒ {name}: Failed - {e}")

def show_urls():
    """Ù†Ù…Ø§ÛŒØ´ URL Ù‡Ø§ÛŒ Ù…Ù‡Ù…"""
    print("\nğŸŒ Available URLs:")
    print("â”€" * 50)
    print("ğŸ  Main Platform:     http://localhost")
    print("ğŸ”§ API Gateway:       http://localhost:8000")
    print("ğŸ¤– LLM Service:       http://localhost:8002")
    print("ğŸ§ª Test Service:      http://localhost:8001") 
    print("ğŸ“Š Prometheus:        http://localhost:9090")
    print("ğŸ“ˆ Grafana:          http://localhost:3000")
    print("   â””â”€ Username: admin")
    print("   â””â”€ Password: admin")
    print("â”€" * 50)

def show_llm_examples():
    """Ù†Ù…Ø§ÛŒØ´ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² LLM"""
    print("\nğŸ¤– LLM Service Examples:")
    print("â”€" * 50)
    print("ğŸ“‹ Model Info:")
    print("   curl http://localhost:8002/model/info")
    print("")
    print("ğŸ’¬ Text Generation:")
    print('   curl -X POST http://localhost:8002/generate \\')
    print('        -H "Content-Type: application/json" \\')
    print('        -d \'{"text": "Ø³Ù„Ø§Ù…", "max_length": 100}\'')
    print("")
    print("ğŸ”— Via API Gateway:")
    print('   curl -X POST http://localhost/api/llm/generate \\')
    print('        -H "Content-Type: application/json" \\')
    print('        -d \'{"text": "Ø³Ù„Ø§Ù…", "max_length": 100}\'')
    print("â”€" * 50)

def show_next_steps():
    """Ù†Ù…Ø§ÛŒØ´ Ù…Ø±Ø§Ø­Ù„ Ø¨Ø¹Ø¯ÛŒ"""
    print("\nğŸ¯ Next Steps:")
    print("â”€" * 50)
    print("1. Check all services: docker compose ps")
    print("2. View logs: docker compose logs -f llm-service")
    print("3. Test LLM: curl http://localhost:8002/model/info")
    print("4. Monitor resources: docker stats")
    print("5. Stop services: docker compose down")
    print("6. Add more AI capabilities!")
    print("â”€" * 50)

def debug_docker_issues():
    """Ø¨Ø±Ø±Ø³ÛŒ Ùˆ Debug Ù…Ø´Ú©Ù„Ø§Øª Docker"""
    print("ğŸ” Debugging Docker issues...")
    
    # Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¶Ø¹ÛŒØª Docker
    try:
        result = subprocess.run(["docker", "info"], capture_output=True, text=True)
        if result.returncode != 0:
            print("âŒ Docker daemon is not running")
            print("   Please start Docker Desktop")
            return False
    except Exception:
        print("âŒ Cannot communicate with Docker")
        return False
    
    # Ø¨Ø±Ø±Ø³ÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø²
    required_files = [
        "docker-compose.yml",
        "services/llm-service/Dockerfile",
        "services/llm-service/requirements.txt",
        "services/llm-service/main.py"
    ]
    
    missing_files = []
    for file_path in required_files:
        if not Path(file_path).exists():
            missing_files.append(file_path)
    
    if missing_files:
        print(f"âŒ Missing required files: {', '.join(missing_files)}")
        return False
    
    print("âœ… Docker and required files are OK")
    return True

def main():
    """ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ"""
    print_banner()
    
    # Ø¨Ø±Ø±Ø³ÛŒ requirements
    if not check_requirements():
        print("âŒ Please install required tools first")
        sys.exit(1)
    
    # Ø¨Ø±Ø±Ø³ÛŒ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ GPU
    check_gpu_support()
    
    # Ø¨Ø±Ø±Ø³ÛŒ Ù…Ù†Ø§Ø¨Ø¹ Ø³ÛŒØ³ØªÙ…
    check_system_resources()
    
    # Ø¨Ø±Ø±Ø³ÛŒ Ù…Ø¯Ù„ LLM
    if not check_llm_model():
        print("âŒ Please ensure LLM model is properly installed")
        sys.exit(1)
    
    # Ø§ÛŒØ¬Ø§Ø¯ Ø¯Ø§ÛŒØ±Ú©ØªÙˆØ±ÛŒâ€ŒÙ‡Ø§
    create_directories()
    
    # ØªÙ†Ø¸ÛŒÙ… environment
    if not setup_environment():
        sys.exit(1)
    
    # Debug Docker issues
    if not debug_docker_issues():
        print("âŒ Please fix Docker issues first")
        sys.exit(1)
    
    # Ø³Ø§Ø®Øª Ø³Ø±ÙˆÛŒØ³â€ŒÙ‡Ø§
    if not build_services():
        print("\nğŸ’¡ Build failed. You can try manual build:")
        print("   docker compose build llm-service --no-cache")
        print("   docker compose build --no-cache")
        sys.exit(1)
    
    # Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø³Ø±ÙˆÛŒØ³â€ŒÙ‡Ø§
    if not start_services():
        sys.exit(1)
    
    # Ø§Ù†ØªØ¸Ø§Ø± Ø¨Ø±Ø§ÛŒ Ø¢Ù…Ø§Ø¯Ù‡ Ø´Ø¯Ù†
    wait_for_services()
    
    # ØªØ³Øª LLM Service
    test_llm_service()
    
    # ØªØ³Øª endpoint Ù‡Ø§
    test_endpoints()
    
    # Ù†Ù…Ø§ÛŒØ´ Ø§Ø·Ù„Ø§Ø¹Ø§Øª
    show_urls()
    show_llm_examples()
    show_next_steps()
    
    print("\nğŸ‰ Setup completed successfully!")
    print("Your Agentic AI Platform with LLM Service is running!")

if __name__ == "__main__":
    main()