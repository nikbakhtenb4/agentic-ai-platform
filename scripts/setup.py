#!/usr/bin/env python3
"""
Agentic AI Platform Setup Script - Enhanced Version (Windows Fixed)
Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ù¾Ù„ØªÙØ±Ù… Ø¨Ø§ LLM Ùˆ STT Services + GPU Sharing
"""

import os
import sys
import subprocess
import time
import requests
from pathlib import Path
import platform
import locale


def set_windows_encoding():
    """ØªÙ†Ø¸ÛŒÙ… encoding Ø¨Ø±Ø§ÛŒ Windows"""
    if platform.system() == "Windows":
        # Set console encoding to UTF-8
        try:
            os.system("chcp 65001 >nul 2>&1")
            # Set environment variables for proper encoding
            os.environ["PYTHONIOENCODING"] = "utf-8"
            os.environ["PYTHONLEGACYWINDOWSSTDIO"] = "1"

            # Try to set locale to UTF-8
            try:
                locale.setlocale(locale.LC_ALL, "en_US.UTF-8")
            except:
                try:
                    locale.setlocale(locale.LC_ALL, "C.UTF-8")
                except:
                    pass

            print("âœ… Windows encoding configured for UTF-8")
            return True
        except Exception as e:
            print(f"âš ï¸  Could not set Windows encoding: {e}")
            return False
    return True


def print_banner():
    """Ù†Ù…Ø§ÛŒØ´ Ø¨Ù†Ø± Ø´Ø±ÙˆØ¹"""
    banner = """
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘        Agentic AI Platform           â•‘
    â•‘   LLM + STT + GPU Sharing Setup     â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """
    print(banner)


def load_environment():
    """Load environment variables from .env file"""
    print("ğŸ”§ Loading environment variables...")

    env_file = Path(".env")
    if not env_file.exists():
        print("âŒ .env file not found!")
        return False

    try:
        with open(env_file, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith("#") and "=" in line:
                    key, value = line.split("=", 1)
                    key = key.strip()
                    value = value.strip().strip('"').strip("'")
                    os.environ[key] = value

        print("âœ… Environment variables loaded successfully")
        return True
    except Exception as e:
        print(f"âŒ Error loading .env file: {e}")
        return False


def check_requirements():
    """Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ Docker Ùˆ Docker Compose"""
    print("ğŸ” Checking requirements...")

    try:
        result = subprocess.run(
            ["docker", "--version"],
            check=True,
            capture_output=True,
            text=True,
            encoding="utf-8",
            errors="ignore",
        )
        docker_version = result.stdout.strip()
        print(f"âœ… Docker: {docker_version}")
    except (subprocess.CalledProcessError, FileNotFoundError):
        print("âŒ Docker is not installed or not in PATH")
        print("ğŸ’¡ Install Docker Desktop: https://docs.docker.com/get-docker/")
        return False

    try:
        result = subprocess.run(
            ["docker", "compose", "version"],
            check=True,
            capture_output=True,
            text=True,
            encoding="utf-8",
            errors="ignore",
        )
        compose_version = result.stdout.strip()
        print(f"âœ… Docker Compose: {compose_version}")
    except (subprocess.CalledProcessError, FileNotFoundError):
        print("âŒ Docker Compose is not installed or not in PATH")
        print("ğŸ’¡ Install Docker Compose: https://docs.docker.com/compose/install/")
        return False

    # Check Docker daemon
    try:
        result = subprocess.run(
            ["docker", "info"],
            capture_output=True,
            text=True,
            timeout=10,
            encoding="utf-8",
            errors="ignore",
        )
        if result.returncode != 0:
            print("âŒ Docker daemon is not running")
            print("ğŸ’¡ Start Docker Desktop or Docker service")
            return False
        print("âœ… Docker daemon is running")
    except subprocess.TimeoutExpired:
        print("âš ï¸  Docker daemon check timed out")
    except Exception as e:
        print(f"âš ï¸  Docker daemon check failed: {e}")

    return True


def check_gpu_support():
    """Ø¨Ø±Ø±Ø³ÛŒ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ GPU Ùˆ ØªÙ†Ø¸ÛŒÙ…Ø§Øª CUDA"""
    print("ğŸ® Checking GPU support and CUDA configuration...")

    gpu_available = False

    try:
        result = subprocess.run(
            ["nvidia-smi"],
            capture_output=True,
            text=True,
            encoding="utf-8",
            errors="ignore",
        )
        if result.returncode == 0:
            print("âœ… NVIDIA GPU detected")

            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø·Ù„Ø§Ø¹Ø§Øª GPU
            lines = result.stdout.split("\n")
            for line in lines:
                if "MiB" in line and "/" in line:
                    print(f"   ğŸ’¾ GPU Memory: {line.strip()}")
                    break

            gpu_available = True
        else:
            print("âš ï¸  No NVIDIA GPU detected")
    except FileNotFoundError:
        print("âš ï¸  nvidia-smi not found")

    # Ø¨Ø±Ø±Ø³ÛŒ Docker GPU runtime
    try:
        result = subprocess.run(
            ["docker", "info"],
            capture_output=True,
            text=True,
            encoding="utf-8",
            errors="ignore",
        )
        if "nvidia" in result.stdout.lower():
            print("âœ… Docker GPU runtime available")
        else:
            print("âš ï¸  Docker GPU runtime not detected")
            if gpu_available:
                print("   ğŸ’¡ Install nvidia-docker for GPU acceleration")
                print(
                    "   ğŸ’¡ Guide: https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html"
                )
    except:
        pass

    if gpu_available:
        print("ğŸš€ GPU acceleration will be used for both LLM and STT services")
        print("ğŸ”„ GPU Coordinator will manage resource sharing")
    else:
        print("ğŸ–¥ï¸  CPU-only mode will be used")

    return gpu_available


def run_docker_command_safe(cmd, timeout=600, ignore_errors=False):
    """
    Ø§Ø¬Ø±Ø§ÛŒ Ø¯Ø³ØªÙˆØ±Ø§Øª Docker Ø¨Ø§ handling Ø¨Ù‡ØªØ± Ø¨Ø±Ø§ÛŒ Windows
    """
    env = os.environ.copy()
    env["PYTHONIOENCODING"] = "utf-8"
    env["DOCKER_BUILDKIT"] = "1"

    # Ø¨Ø±Ø§ÛŒ Windows Ø§Ø² shell=True Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
    shell_mode = platform.system() == "Windows"

    try:
        # Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² timeoutØŒ Ø§Ø² Popen Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
        process = subprocess.Popen(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,  # ØªØ±Ú©ÛŒØ¨ stderr Ø¨Ø§ stdout
            text=True,
            encoding="utf-8",
            errors="ignore",  # ignore encoding errors
            shell=shell_mode,
            env=env,
            bufsize=1,
            universal_newlines=True,
        )

        # Ø®ÙˆØ§Ù†Ø¯Ù† output Ø¨Ù‡ ØµÙˆØ±Øª real-time
        output_lines = []
        while True:
            output = process.stdout.readline()
            if output == "" and process.poll() is not None:
                break
            if output:
                output_lines.append(output.strip())
                # Ù†Ù…Ø§ÛŒØ´ progress Ø¨Ø±Ø§ÛŒ build Ù‡Ø§ÛŒ Ø·ÙˆÙ„Ø§Ù†ÛŒ
                if any(
                    keyword in output.lower()
                    for keyword in ["downloading", "extracting", "pulling"]
                ):
                    print(f"   ğŸ“¥ {output.strip()[:80]}...")
                elif "error" in output.lower() and not ignore_errors:
                    print(f"   âŒ {output.strip()}")

        # Ø§Ù†ØªØ¸Ø§Ø± Ø¨Ø±Ø§ÛŒ Ù¾Ø§ÛŒØ§Ù† process
        return_code = process.poll()

        result_output = "\n".join(output_lines)

        if return_code == 0:
            return True, result_output
        else:
            return False, result_output

    except Exception as e:
        return False, f"Command execution failed: {e}"


def build_services():
    """Ø³Ø§Ø®Øª Ø³Ø±ÙˆÛŒØ³â€ŒÙ‡Ø§ Ø¨Ø§ error handling Ø¨Ù‡ØªØ± Ø¨Ø±Ø§ÛŒ Windows"""
    print("ğŸ—ï¸  Building services with GPU sharing support...")

    # Ù„ÛŒØ³Øª Ø³Ø±ÙˆÛŒØ³â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ build
    services_to_build = [
        ("test-service", "ğŸ§ª Test Service"),
        ("gpu-coordinator", "ğŸ”„ GPU Coordinator"),
        ("llm-service", "ğŸ¤– LLM Service"),
        ("stt-service", "ğŸ™ï¸  STT Service"),
        ("api-gateway", "ğŸŒ API Gateway"),
    ]

    successful_builds = []
    failed_builds = []

    for service_name, display_name in services_to_build:
        print(f"\n{display_name}...")

        # Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ Dockerfile
        dockerfile_paths = [
            f"services/{service_name}/Dockerfile",
            f"services/audio-service/stt/Dockerfile"
            if service_name == "stt-service"
            else None,
        ]

        dockerfile_path = None
        for path in dockerfile_paths:
            if path and Path(path).exists():
                dockerfile_path = Path(path)
                break

        if not dockerfile_path:
            print(f"âš ï¸  Dockerfile not found for {service_name}, skipping...")
            continue

        # Build service Ø¨Ø§ Windows-safe approach
        build_success = False

        print(f"   ğŸ”¨ Building {service_name}...")

        # Strategy 1: Normal build
        success, output = run_docker_command_safe(
            ["docker", "compose", "build", service_name], timeout=900
        )

        if success:
            print(f"âœ… {display_name} built successfully")
            successful_builds.append(service_name)
            build_success = True
        else:
            print(f"âš ï¸  {display_name} build failed, trying with --no-cache...")

            # Strategy 2: Build with --no-cache
            success, output = run_docker_command_safe(
                ["docker", "compose", "build", service_name, "--no-cache"], timeout=1200
            )

            if success:
                print(f"âœ… {display_name} built successfully (no-cache)")
                successful_builds.append(service_name)
                build_success = True
            else:
                print(f"âŒ {display_name} build failed")
                # Ù†Ù…Ø§ÛŒØ´ Ø®Ø·Ø§ (ÙÙ‚Ø· Ø¢Ø®Ø±ÛŒÙ† Ø®Ø·ÙˆØ·)
                error_lines = output.split("\n")[-5:]
                for line in error_lines:
                    if line.strip() and "error" in line.lower():
                        print(f"   ğŸ” {line.strip()}")

        if not build_success:
            failed_builds.append((service_name, display_name))

    # Summary
    print(f"\nğŸ“Š Build Summary:")
    print(f"   âœ… Successful: {len(successful_builds)} services")
    print(f"   âŒ Failed: {len(failed_builds)} services")

    if successful_builds:
        print(f"   Built services: {', '.join(successful_builds)}")

    if failed_builds:
        print(f"   Failed services: {', '.join([s[0] for s in failed_builds])}")
        print("\nğŸ’¡ Manual build suggestions:")
        for service_name, display_name in failed_builds:
            print(f"   docker compose build {service_name} --no-cache --progress=plain")

        # If critical services failed, offer to continue anyway
        critical_services = ["api-gateway", "llm-service"]
        critical_failed = [s for s in failed_builds if s[0] in critical_services]

        if critical_failed:
            print(f"\nâš ï¸  Critical services failed: {[s[0] for s in critical_failed]}")
            response = input("Continue with partial setup? (y/N): ").lower().strip()
            if response != "y":
                return False

    return len(successful_builds) > 0


def start_services():
    """Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø³Ø±ÙˆÛŒØ³â€ŒÙ‡Ø§"""
    print("ğŸš€ Starting services...")

    # Ø§Ø¨ØªØ¯Ø§ infrastructure services
    infrastructure_services = ["postgres", "redis"]

    print("ğŸ—„ï¸  Starting infrastructure services...")
    for service in infrastructure_services:
        success, output = run_docker_command_safe(
            ["docker", "compose", "up", "-d", service], timeout=120
        )

        if success:
            print(f"âœ… {service.title()} started")
            time.sleep(2)
        else:
            print(f"âš ï¸  {service.title()} start failed")

    # Ø³Ù¾Ø³ GPU Coordinator
    print("ğŸ”„ Starting GPU Coordinator...")
    success, output = run_docker_command_safe(
        ["docker", "compose", "up", "-d", "gpu-coordinator"], timeout=120
    )

    if success:
        print("âœ… GPU Coordinator started")
        time.sleep(5)  # Ø§Ù†ØªØ¸Ø§Ø± Ø¨Ø±Ø§ÛŒ Ø¢Ù…Ø§Ø¯Ù‡ Ø´Ø¯Ù†
    else:
        print("âš ï¸  GPU Coordinator start failed, continuing...")

    # Ø´Ø±ÙˆØ¹ Ù‡Ù…Ù‡ Ø³Ø±ÙˆÛŒØ³â€ŒÙ‡Ø§
    print("ğŸŒ Starting all services...")
    success, output = run_docker_command_safe(
        ["docker", "compose", "up", "-d"], timeout=180
    )

    if success:
        print("âœ… All services startup command executed")

        # Ù†Ù…Ø§ÛŒØ´ ÙˆØ¶Ø¹ÛŒØª containers
        time.sleep(3)
        success, ps_output = run_docker_command_safe(
            ["docker", "compose", "ps"], timeout=30
        )

        if success:
            print("ğŸ“Š Container status:")
            print(ps_output)

        return True
    else:
        print("âŒ Failed to start services")
        print("ğŸ” Checking for failed containers...")
        return False


def check_llm_model():
    """Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ Ù…Ø¯Ù„ LLM"""
    print("ğŸ¤– Checking LLM model...")

    # Ù…Ø³ÛŒØ± Ù…Ø¯Ù„ Ø§Ø² environment variable ÛŒØ§ default
    model_path_env = os.getenv("MODEL_PATH", "/app/models/gpt2-fa")
    local_model_path = model_path_env.replace("/app/", "data/")

    model_path = Path(local_model_path)
    if not model_path.exists():
        print(f"âŒ LLM model directory not found!")
        print(f"   Expected path: {model_path.absolute()}")
        print("   Please ensure the GPT2-FA model is placed in the correct directory")
        print("   ğŸ’¡ Download from: https://huggingface.co/YOUR_MODEL_NAME")
        return False

    # Ø¨Ø±Ø±Ø³ÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø² Ù…Ø¯Ù„
    required_files = [
        "config.json",
        "pytorch_model.bin",
        "tokenizer.json",
        "vocab.json",
    ]

    missing_files = []
    existing_files = []

    for file_name in required_files:
        file_path = model_path / file_name
        if file_path.exists():
            existing_files.append(file_name)
        else:
            missing_files.append(file_name)

    print(f"   âœ… Found {len(existing_files)} model files: {', '.join(existing_files)}")

    if missing_files:
        print(f"   âš ï¸  Missing files: {', '.join(missing_files)}")
        print("   The model might still work, but please verify completeness")
    else:
        print("âœ… All essential LLM model files found")

    # Ø¨Ø±Ø±Ø³ÛŒ Ø­Ø¬Ù… Ù…Ø¯Ù„
    try:
        total_size = sum(f.stat().st_size for f in model_path.rglob("*") if f.is_file())
        size_mb = total_size / (1024 * 1024)
        print(f"ğŸ“¦ LLM Model size: {size_mb:.1f} MB")

        if size_mb < 10:
            print("âš ï¸  LLM Model size seems small - please verify model integrity")
        elif size_mb > 5000:
            print("ğŸ’¾ Large model detected - ensure sufficient system memory")

    except Exception as e:
        print(f"âš ï¸  Could not calculate LLM model size: {e}")

    return True


def check_stt_model():
    """Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ Ùˆ Ø¢Ù…Ø§Ø¯Ú¯ÛŒ Ù…Ø¯Ù„ STT (Whisper)"""
    print("ğŸ™ï¸  Checking STT model configuration...")

    # Ù…Ø³ÛŒØ± Ù…Ø¯Ù„ STT
    stt_model_path_env = os.getenv("WHISPER_MODEL_PATH", "/app/models/stt")
    local_stt_path = stt_model_path_env.replace("/app/", "data/")

    stt_model_path = Path(local_stt_path)
    whisper_model_size = os.getenv("WHISPER_MODEL_SIZE", "medium")

    if stt_model_path.exists():
        # Ø¨Ø±Ø±Ø³ÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Whisper Ù…ÙˆØ¬ÙˆØ¯
        whisper_files = list(stt_model_path.glob("*.pt"))
        if whisper_files:
            print(f"âœ… Found {len(whisper_files)} Whisper model files")
            for file in whisper_files:
                size_mb = file.stat().st_size / (1024 * 1024)
                print(f"   ğŸ“ {file.name}: {size_mb:.1f} MB")
        else:
            print("ğŸ“¥ STT model directory exists but no .pt files found")
            print("   Whisper models will be downloaded automatically on first use")
    else:
        print("ğŸ“¥ STT model directory not found")
        print(
            f"   Will be created and Whisper '{whisper_model_size}' model will be downloaded"
        )

    print(f"ğŸ”§ Configured Whisper model size: {whisper_model_size}")

    # ØªØ®Ù…ÛŒÙ† Ø­Ø¬Ù… Ø¯Ø§Ù†Ù„ÙˆØ¯
    model_sizes = {"tiny": 39, "base": 74, "small": 244, "medium": 769, "large": 1550}

    estimated_size = model_sizes.get(whisper_model_size, 500)
    print(f"ğŸ“Š Estimated download size: ~{estimated_size}MB")

    # Ø¨Ø±Ø±Ø³ÛŒ ØªÙ†Ø¸ÛŒÙ…Ø§Øª STT
    max_file_size = os.getenv("MAX_FILE_SIZE_MB", "25")
    supported_langs = os.getenv("SUPPORTED_LANGUAGES", "fa,en")
    print(f"ğŸ“Š Max file size: {max_file_size}MB")
    print(f"ğŸŒ Supported languages: {supported_langs}")

    return True


def create_directories():
    """Ø§ÛŒØ¬Ø§Ø¯ Ø¯Ø§ÛŒØ±Ú©ØªÙˆØ±ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø²"""
    print("ğŸ“ Creating required directories...")

    directories = [
        "data/models/llm",
        "data/models/stt",
        "data/vectors",
        "data/cache",
        "data/logs",
        "data/uploads",
        "monitoring/grafana/dashboards",
        "monitoring/grafana/provisioning/datasources",
        "services/api-gateway/middleware",
        "services/api-gateway/routes",
        "services/test-service",
        "services/llm-service/models",
        "services/llm-service/utils",
        "services/audio-service/stt",
        "services/gpu-coordinator",
        "shared/database",
        "shared/models",
        "shared/utils",
    ]

    created_count = 0
    for directory in directories:
        try:
            Path(directory).mkdir(parents=True, exist_ok=True)
            created_count += 1
        except Exception as e:
            print(f"âš ï¸  Could not create {directory}: {e}")

    print(f"âœ… Created {created_count}/{len(directories)} directories")


def setup_environment():
    """ØªÙ†Ø¸ÛŒÙ… Ùˆ validation environment variables"""
    print("ğŸ”§ Validating environment configuration...")

    # Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù„ÛŒØ¯Ù‡Ø§ÛŒ Ù…Ù‡Ù… environment
    important_keys = {
        "MODEL_PATH": "/app/models/gpt2-fa",
        "LLM_SERVICE_PORT": "8002",
        "STT_SERVICE_PORT": "8003",
        "GPU_COORDINATOR_PORT": "8080",
        "WHISPER_MODEL_SIZE": "medium",
        "CUDA_VISIBLE_DEVICES": "0",
        "MAX_MEMORY_MB": "6144",
    }

    print("ğŸ” Environment variables status:")
    all_set = True

    for key, default_value in important_keys.items():
        value = os.getenv(key)
        if value:
            print(f"   âœ… {key}: {value}")
        else:
            print(f"   âš ï¸  {key}: not set, using default ({default_value})")
            os.environ[key] = default_value
            all_set = False

    if all_set:
        print("âœ… All environment variables properly configured")
    else:
        print("âš ï¸  Some environment variables were missing - defaults applied")

    return True


def check_system_resources():
    """Ø¨Ø±Ø±Ø³ÛŒ Ù…Ù†Ø§Ø¨Ø¹ Ø³ÛŒØ³ØªÙ…"""
    print("ğŸ’¾ Checking system resources...")

    try:
        import psutil

        # Ø¨Ø±Ø±Ø³ÛŒ RAM
        memory = psutil.virtual_memory()
        memory_gb = memory.total / (1024**3)
        available_gb = memory.available / (1024**3)

        print(f"ğŸ Total RAM: {memory_gb:.1f} GB")
        print(f"ğŸ Available RAM: {available_gb:.1f} GB")

        if memory_gb < 8:
            print("âš ï¸  Warning: Less than 8GB RAM detected")
            print("   Both LLM and STT services will compete for memory")
            print("   ğŸ’¡ Consider using CPU-only mode or lighter models")
        elif memory_gb >= 16:
            print("âœ… Excellent RAM for both LLM and STT processing")
        else:
            print("âœ… Adequate RAM - GPU Coordinator will manage resources")

        # Ø¨Ø±Ø±Ø³ÛŒ CPU
        cpu_count = psutil.cpu_count(logical=False)
        cpu_logical = psutil.cpu_count(logical=True)
        print(f"ğŸ”§ CPU cores: {cpu_count} physical, {cpu_logical} logical")

        if cpu_count < 4:
            print("âš ï¸  Consider using GPU acceleration for better performance")

        # Ø¨Ø±Ø±Ø³ÛŒ ÙØ¶Ø§ÛŒ Ø¯ÛŒØ³Ú©
        try:
            current_dir = os.path.abspath(".")
            disk = psutil.disk_usage(current_dir)
            free_gb = disk.free / (1024**3)
            total_gb = disk.total / (1024**3)

            print(f"ğŸ’¿ Disk space: {free_gb:.1f} GB free of {total_gb:.1f} GB total")

            if free_gb < 10:
                print("âš ï¸  Warning: Low disk space")
                print("   Whisper models and LLM models require significant space")
                print(
                    "   ğŸ’¡ Consider freeing up at least 10GB for models and containers"
                )
            elif free_gb < 5:
                print("âŒ Critical: Very low disk space")
                print("   Setup may fail due to insufficient space")
                return False

        except Exception as disk_error:
            print(f"âš ï¸  Could not check disk space: {disk_error}")

    except ImportError:
        print("âš ï¸  psutil not available - skipping detailed resource check")
        print("   ğŸ’¡ Install with: pip install psutil")
    except Exception as e:
        print(f"âš ï¸  Error checking system resources: {e}")

    return True


def wait_for_services():
    """Ø§Ù†ØªØ¸Ø§Ø± Ø¨Ø±Ø§ÛŒ Ø¢Ù…Ø§Ø¯Ù‡ Ø´Ø¯Ù† Ø³Ø±ÙˆÛŒØ³â€ŒÙ‡Ø§"""
    print("â³ Waiting for services to be ready...")

    services = {
        "GPU Coordinator": ("http://localhost:8080/health", 30),
        "API Gateway": ("http://localhost:8000/health", 30),
        "LLM Service": ("http://localhost:8002/health", 90),
        "STT Service": ("http://localhost:8003/health", 90),
        "Test Service": ("http://localhost:8001/health", 30),
        "Prometheus": ("http://localhost:9090/-/ready", 30),
        "Grafana": ("http://localhost:3000/api/health", 30),
    }

    ready_services = []
    failed_services = []

    for service_name, (url, max_attempts) in services.items():
        print(f"ğŸ” Checking {service_name}...")

        service_ready = False
        for attempt in range(max_attempts):
            try:
                response = requests.get(url, timeout=5)
                if response.status_code == 200:
                    print(f"âœ… {service_name} is ready")
                    ready_services.append(service_name)
                    service_ready = True
                    break
            except requests.exceptions.RequestException:
                pass

            if attempt < max_attempts - 1:
                if service_name in ["LLM Service", "STT Service"] and attempt % 15 == 0:
                    print(f"   ğŸ’­ Loading AI model... ({attempt}/{max_attempts})")
                elif attempt % 10 == 0:
                    print(f"   â³ Waiting... ({attempt}/{max_attempts})")
                time.sleep(2)

        if not service_ready:
            print(f"âš ï¸  {service_name} not ready after {max_attempts} attempts")
            failed_services.append(service_name)

    print(f"\nğŸ“Š Service Status Summary:")
    print(f"   âœ… Ready: {len(ready_services)} services")
    print(f"   âš ï¸  Not ready: {len(failed_services)} services")

    if ready_services:
        print(f"   Ready services: {', '.join(ready_services)}")

    if failed_services:
        print(f"   Failed services: {', '.join(failed_services)}")
        print("   ğŸ’¡ Check logs with: docker compose logs [service-name]")


def test_gpu_coordinator():
    """ØªØ³Øª GPU Coordinator"""
    print("ğŸ”„ Testing GPU Coordinator...")

    try:
        # ØªØ³Øª health endpoint
        response = requests.get("http://localhost:8080/health", timeout=10)
        if response.status_code == 200:
            health_data = response.json()
            print("âœ… GPU Coordinator health check passed")
            print(f"   ğŸ® GPU Available: {health_data.get('gpu_available', False)}")
        else:
            print(f"âš ï¸  GPU Coordinator health check returned: {response.status_code}")
            return False

        # ØªØ³Øª status endpoint
        response = requests.get("http://localhost:8080/status", timeout=10)
        if response.status_code == 200:
            status_data = response.json()
            print(f"âœ… GPU Status retrieved")
            print(
                f"   ğŸ“Š Available Memory: {status_data.get('available_memory_mb', 'Unknown')} MB"
            )
            print(f"   ğŸ”§ Device Count: {status_data.get('device_count', 'Unknown')}")
        else:
            print("âš ï¸  Could not retrieve GPU status")

        # ØªØ³Øª queue status
        response = requests.get("http://localhost:8080/queue", timeout=10)
        if response.status_code == 200:
            queue_data = response.json()
            print("âœ… Queue status retrieved")
            print(
                f"   ğŸ“‹ Pending requests: {queue_data.get('pending_requests', {}).get('total', 0)}"
            )

        return True

    except requests.exceptions.RequestException as e:
        print(f"âŒ GPU Coordinator test failed: {e}")
        return False


def test_llm_service():
    """ØªØ³Øª LLM Service"""
    print("ğŸ¤– Testing LLM Service...")

    try:
        # ØªØ³Øª health endpoint
        response = requests.get("http://localhost:8002/health", timeout=10)
        if response.status_code == 200:
            print("âœ… LLM Health check passed")
        else:
            print(f"âš ï¸  LLM Health check returned: {response.status_code}")
            return False

        # ØªØ³Øª model info endpoint
        response = requests.get("http://localhost:8002/model/info", timeout=10)
        if response.status_code == 200:
            model_info = response.json()
            print(f"âœ… LLM Model loaded: {model_info.get('model_name', 'Unknown')}")
            print(f"   ğŸ“Š Max length: {model_info.get('max_length', 'Unknown')}")
            print(f"   ğŸ® Device: {model_info.get('device', 'Unknown')}")
        else:
            print("âš ï¸  Could not retrieve LLM model info")

        # ØªØ³Øª Ø³Ø§Ø¯Ù‡ ØªÙˆÙ„ÛŒØ¯ Ù…ØªÙ†
        test_payload = {"text": "Ø³Ù„Ø§Ù…", "max_length": 50, "temperature": 0.7}

        print("ğŸ”¬ Testing LLM text generation...")
        response = requests.post(
            "http://localhost:8002/generate", json=test_payload, timeout=30
        )

        if response.status_code == 200:
            result = response.json()
            generated_text = result.get("generated_text", "")[:100]
            print(f"âœ… LLM text generation test passed")
            print(f"   ğŸ’¬ Sample output: {generated_text}...")
        else:
            print(f"âš ï¸  LLM text generation test failed: {response.status_code}")

        return True

    except requests.exceptions.RequestException as e:
        print(f"âŒ LLM Service test failed: {e}")
        return False


def test_stt_service():
    """ØªØ³Øª STT Service"""
    print("ğŸ™ï¸  Testing STT Service...")

    try:
        # ØªØ³Øª health endpoint
        response = requests.get("http://localhost:8003/health", timeout=10)
        if response.status_code == 200:
            print("âœ… STT Health check passed")
        else:
            print(f"âš ï¸  STT Health check returned: {response.status_code}")
            return False

        # ØªØ³Øª model info endpoint
        response = requests.get("http://localhost:8003/model/info", timeout=10)
        if response.status_code == 200:
            model_info = response.json()
            print(f"âœ… STT Model info: {model_info.get('model_name', 'Unknown')}")
            print(
                f"   ğŸŒ Supported languages: {model_info.get('supported_languages', 'Unknown')}"
            )
            print(f"   ğŸ® Device: {model_info.get('device', 'Unknown')}")
        else:
            print("âš ï¸  Could not retrieve STT model info")

        # ØªØ³Øª supported formats
        response = requests.get("http://localhost:8003/formats", timeout=10)
        if response.status_code == 200:
            formats = response.json()
            supported_formats = formats.get("supported_formats", [])
            print(f"âœ… Supported audio formats: {supported_formats}")

        return True

    except requests.exceptions.RequestException as e:
        print(f"âŒ STT Service test failed: {e}")
        return False


def test_endpoints():
    """ØªØ³Øª endpoint Ù‡Ø§ÛŒ Ø§ØµÙ„ÛŒ"""
    print("ğŸ§ª Testing main endpoints...")

    endpoints = [
        ("Root", "http://localhost/"),
        ("API Info", "http://localhost/api/info"),
        ("Health Check", "http://localhost/health"),
        ("Test Service", "http://localhost/test/ping"),
        ("GPU Coordinator via Gateway", "http://localhost/api/gpu/status"),
        ("LLM via Gateway", "http://localhost/api/llm/health"),
        ("STT via Gateway", "http://localhost/api/stt/health"),
    ]

    working_endpoints = []
    failed_endpoints = []

    for name, url in endpoints:
        try:
            response = requests.get(url, timeout=5)
            if response.status_code == 200:
                print(f"âœ… {name}: OK")
                working_endpoints.append(name)
            else:
                print(f"âš ï¸  {name}: Status {response.status_code}")
                failed_endpoints.append(name)
        except requests.exceptions.RequestException as e:
            print(f"âŒ {name}: Failed - {type(e).__name__}")
            failed_endpoints.append(name)

    print(f"\nğŸ“Š Endpoint Test Summary:")
    print(f"   âœ… Working: {len(working_endpoints)}/{len(endpoints)}")
    if failed_endpoints:
        print(f"   âŒ Failed: {', '.join(failed_endpoints)}")


def show_urls():
    """Ù†Ù…Ø§ÛŒØ´ URL Ù‡Ø§ÛŒ Ù…Ù‡Ù…"""
    print("\nğŸŒ Available URLs:")
    print("â”€" * 60)
    print("ğŸ  Main Platform:     http://localhost")
    print("ğŸ”§ API Gateway:       http://localhost:8000")
    print("ğŸ”„ GPU Coordinator:   http://localhost:8080")
    print("ğŸ¤– LLM Service:       http://localhost:8002")
    print("ğŸ™ï¸  STT Service:       http://localhost:8003")
    print("ğŸ§ª Test Service:      http://localhost:8001")
    print("ğŸ“Š Prometheus:        http://localhost:9090")
    print("ğŸ“ˆ Grafana:          http://localhost:3000")
    print("   â””â”€ Username: admin")
    print("   â””â”€ Password: admin")
    print("â”€" * 60)


def show_ai_examples():
    """Ù†Ù…Ø§ÛŒØ´ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² AI Services"""
    print("\nğŸ¤– AI Services Examples:")
    print("â”€" * 60)
    print("ğŸ”„ GPU Status:")
    print("   curl http://localhost:8080/status")
    print("")
    print("ğŸ“‹ LLM Model Info:")
    print("   curl http://localhost:8002/model/info")
    print("")
    print("ğŸ’¬ LLM Text Generation:")
    print("   curl -X POST http://localhost:8002/generate \\")
    print('        -H "Content-Type: application/json" \\')
    print('        -d \'{"text": "Ø³Ù„Ø§Ù…", "max_length": 100}\'')
    print("")
    print("ğŸ™ï¸  STT Model Info:")
    print("   curl http://localhost:8003/model/info")
    print("")
    print("ğŸ”Š STT Audio Transcription:")
    print("   curl -X POST http://localhost:8003/transcribe \\")
    print('        -F "audio=@audio_file.wav" \\')
    print('        -F "language=fa"')
    print("")
    print("ğŸ”— Via API Gateway:")
    print("   curl -X POST http://localhost/api/llm/generate \\")
    print('        -H "Content-Type: application/json" \\')
    print('        -d \'{"text": "Ø³Ù„Ø§Ù…", "max_length": 100}\'')
    print("â”€" * 60)


def show_troubleshooting():
    """Ù†Ù…Ø§ÛŒØ´ Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ troubleshooting"""
    print("\nğŸ”§ Troubleshooting Guide:")
    print("â”€" * 60)
    print("ğŸ“Š Check container status:")
    print("   docker compose ps")
    print("")
    print("ğŸ“‹ View logs:")
    print("   docker compose logs -f [service-name]")
    print("   docker compose logs -f llm-service")
    print("   docker compose logs -f stt-service")
    print("   docker compose logs -f gpu-coordinator")
    print("")
    print("ğŸ”„ Restart specific service:")
    print("   docker compose restart [service-name]")
    print("")
    print("ğŸ› ï¸  Rebuild and restart:")
    print("   docker compose down")
    print("   docker compose build [service-name] --no-cache")
    print("   docker compose up -d")
    print("")
    print("ğŸ’¾ Check resources:")
    print("   docker stats")
    print("   nvidia-smi  # for GPU usage")
    print("")
    print("ğŸ§¹ Clean up (if needed):")
    print("   docker compose down -v")
    print("   docker system prune -f")
    print("")
    print("ğŸªŸ Windows specific issues:")
    print("   â€¢ Run PowerShell/CMD as Administrator")
    print("   â€¢ Ensure Docker Desktop is running")
    print("   â€¢ Check Windows firewall settings")
    print("   â€¢ Restart Docker Desktop if needed")
    print("â”€" * 60)


def show_next_steps():
    """Ù†Ù…Ø§ÛŒØ´ Ù…Ø±Ø§Ø­Ù„ Ø¨Ø¹Ø¯ÛŒ"""
    print("\nğŸ¯ Next Steps:")
    print("â”€" * 60)
    print("1. Test AI capabilities:")
    print("   â€¢ LLM: curl http://localhost:8002/model/info")
    print("   â€¢ STT: curl http://localhost:8003/model/info")
    print("   â€¢ GPU: curl http://localhost:8080/status")
    print("")
    print("2. Monitor system:")
    print("   â€¢ Container health: docker compose ps")
    print("   â€¢ Resource usage: docker stats")
    print("   â€¢ GPU usage: nvidia-smi")
    print("")
    print("3. Scale and optimize:")
    print("   â€¢ Adjust memory limits in .env")
    print("   â€¢ Monitor GPU sharing efficiency")
    print("   â€¢ Add more AI models as needed")
    print("")
    print("4. Production preparation:")
    print("   â€¢ Change default passwords")
    print("   â€¢ Configure SSL/TLS")
    print("   â€¢ Set up proper monitoring")
    print("   â€¢ Implement backup strategies")
    print("")
    print("5. Windows optimization:")
    print("   â€¢ Consider WSL2 for better performance")
    print("   â€¢ Monitor Windows resource usage")
    print("   â€¢ Keep Docker Desktop updated")
    print("â”€" * 60)


def create_windows_env_file():
    """Ø§ÛŒØ¬Ø§Ø¯ ÙØ§ÛŒÙ„ .env Ø³Ø§Ø²Ú¯Ø§Ø± Ø¨Ø§ Windows"""
    print("ğŸ“ Creating Windows-compatible .env file...")

    env_content = """# Agentic AI Platform Configuration - Windows Compatible
# GPU and Memory Settings
CUDA_VISIBLE_DEVICES=0
MAX_MEMORY_MB=6144
GPU_MEMORY_FRACTION=0.8

# Model Paths (Windows compatible)
MODEL_PATH=/app/models/gpt2-fa
WHISPER_MODEL_PATH=/app/models/stt
WHISPER_MODEL_SIZE=medium

# Service Ports
API_GATEWAY_PORT=8000
LLM_SERVICE_PORT=8002
STT_SERVICE_PORT=8003
GPU_COORDINATOR_PORT=8080
TEST_SERVICE_PORT=8001

# Database Configuration
POSTGRES_DB=agentic_platform
POSTGRES_USER=postgres
POSTGRES_PASSWORD=postgres123
POSTGRES_HOST=postgres
POSTGRES_PORT=5432

# Redis Configuration
REDIS_HOST=redis
REDIS_PORT=6379
REDIS_PASSWORD=redis123

# STT Configuration
MAX_FILE_SIZE_MB=25
SUPPORTED_LANGUAGES=fa,en
AUDIO_TEMP_DIR=/tmp/audio

# Monitoring
PROMETHEUS_PORT=9090
GRAFANA_PORT=3000

# Windows Specific
DOCKER_BUILDKIT=1
PYTHONIOENCODING=utf-8
COMPOSE_CONVERT_WINDOWS_PATHS=1

# Application Settings
DEBUG=false
LOG_LEVEL=INFO
"""

    env_file = Path(".env")
    if not env_file.exists():
        try:
            with open(env_file, "w", encoding="utf-8") as f:
                f.write(env_content)
            print("âœ… Created .env file with Windows-compatible settings")
            return True
        except Exception as e:
            print(f"âŒ Failed to create .env file: {e}")
            return False
    else:
        print("âœ… .env file already exists")
        return True


def main():
    """ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ"""
    # ØªÙ†Ø¸ÛŒÙ… encoding Ø¨Ø±Ø§ÛŒ Windows
    set_windows_encoding()

    print_banner()

    # Create .env file if it doesn't exist
    if not Path(".env").exists():
        create_windows_env_file()

    # Load environment variables
    if not load_environment():
        print("âŒ Environment setup failed")
        sys.exit(1)

    # Ø¨Ø±Ø±Ø³ÛŒ requirements
    if not check_requirements():
        print("âŒ Please install required tools first")
        print("ğŸ’¡ Make sure Docker Desktop is running")
        sys.exit(1)

    # Ø¨Ø±Ø±Ø³ÛŒ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ GPU
    gpu_available = check_gpu_support()

    # Ø¨Ø±Ø±Ø³ÛŒ Ù…Ù†Ø§Ø¨Ø¹ Ø³ÛŒØ³ØªÙ…
    if not check_system_resources():
        print("âŒ Insufficient system resources")
        print("ğŸ’¡ Consider closing other applications to free up resources")
        response = input("Continue anyway? (y/N): ").lower().strip()
        if response != "y":
            sys.exit(1)

    # Ø¨Ø±Ø±Ø³ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ AI
    if not check_llm_model():
        print("âŒ Please ensure LLM model is properly installed")
        print("ğŸ’¡ Check the data/models/llm/gpt2-fa/ directory")
        response = input("Continue without LLM model? (y/N): ").lower().strip()
        if response != "y":
            sys.exit(1)

    check_stt_model()  # STT Ù…Ø¯Ù„ Ø§Ø®ØªÛŒØ§Ø±ÛŒ Ø§Ø³Øª

    # Ø§ÛŒØ¬Ø§Ø¯ Ø¯Ø§ÛŒØ±Ú©ØªÙˆØ±ÛŒâ€ŒÙ‡Ø§
    create_directories()

    # ØªÙ†Ø¸ÛŒÙ… environment
    if not setup_environment():
        sys.exit(1)

    print(f"\nğŸ–¥ï¸  Platform: {platform.system()} {platform.release()}")
    if platform.system() == "Windows":
        print("ğŸªŸ Windows-specific optimizations applied")

    # Ø³Ø§Ø®Øª Ø³Ø±ÙˆÛŒØ³â€ŒÙ‡Ø§
    if not build_services():
        print("\nğŸ’¡ Build failed. Troubleshooting suggestions:")
        print("   1. Check your internet connection")
        print("   2. Restart Docker Desktop")
        print("   3. Run as Administrator")
        print("   4. Clear Docker cache: docker system prune -f")
        print("   5. Manual build: docker compose build --no-cache")

        response = input("\nContinue with partial setup? (y/N): ").lower().strip()
        if response != "y":
            print("ğŸ’¡ Fix the build issues and run the script again")
            sys.exit(1)

    # Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø³Ø±ÙˆÛŒØ³â€ŒÙ‡Ø§
    if not start_services():
        print("âŒ Service startup failed")
        print("ğŸ’¡ Check Docker Desktop and try again")
        print("ğŸ’¡ Run: docker compose logs to see detailed errors")

    # Ø§Ù†ØªØ¸Ø§Ø± Ø¨Ø±Ø§ÛŒ Ø¢Ù…Ø§Ø¯Ù‡ Ø´Ø¯Ù†
    wait_for_services()

    # ØªØ³Øª Ø³Ø±ÙˆÛŒØ³â€ŒÙ‡Ø§
    print("\nğŸ§ª Running service tests...")
    test_gpu_coordinator()
    test_llm_service()
    test_stt_service()

    # ØªØ³Øª endpoint Ù‡Ø§
    test_endpoints()

    # Ù†Ù…Ø§ÛŒØ´ Ø§Ø·Ù„Ø§Ø¹Ø§Øª
    show_urls()
    show_ai_examples()
    show_troubleshooting()
    show_next_steps()

    print("\nğŸ‰ Setup completed!")
    print("Your Agentic AI Platform with GPU Sharing is running!")
    print("ğŸ¤– LLM Service: Ready for text generation")
    print("ğŸ™ï¸  STT Service: Ready for speech recognition")
    print("ğŸ”„ GPU Coordinator: Managing resource sharing")

    # Final status check
    try:
        success, ps_output = run_docker_command_safe(
            ["docker", "compose", "ps"], timeout=30
        )

        if success:
            running_containers = ps_output.count("Up")
            total_lines = len([line for line in ps_output.split("\n") if line.strip()])
            total_containers = max(0, total_lines - 1)  # exclude header

            if running_containers > 0:
                print(
                    f"\nğŸ“Š Final Status: {running_containers}/{total_containers} containers running"
                )

            # Show any failed containers
            if "Exit" in ps_output or "Restarting" in ps_output:
                print("âš ï¸  Some containers may have issues:")
                print("   Run: docker compose logs [service-name] for details")

    except Exception:
        print("âš ï¸  Could not get final container status")

    print("\nğŸ’¡ For Windows users:")
    print("   â€¢ Keep Docker Desktop running")
    print("   â€¢ Monitor resource usage in Task Manager")
    print("   â€¢ Consider WSL2 for better Docker performance")


if __name__ == "__main__":
    main()
